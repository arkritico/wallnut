# ðŸ¤– Autonomous CYPE Scraper

Comprehensive background scraping system for completing CYPE price database coverage.

## Overview

The autonomous scraper focuses on items that haven't been scraped yet, running sequentially in the background without requiring user interaction.

**Current Status:**
- âœ… 2,049 items already scraped
- ðŸ“‹ 194 items remaining
- ðŸ“Š 92% coverage

## Features

- **ðŸŽ¯ Targeted**: Only scrapes missing items
- **ðŸ¤– Autonomous**: No user interaction needed
- **ðŸ’¾ Incremental Saves**: Saves progress every 10 items
- **ðŸ”„ Resumable**: Can be stopped and resumed anytime
- **ðŸ“Š Monitoring**: Real-time progress tracking
- **ðŸ“ Logging**: Complete activity log
- **âš¡ Auto-Selection**: Automatically picks first option for dropdowns
- **ðŸ” Retry Logic**: Retries failed items twice

## Quick Start

### 1. Find Missing Items

```bash
npm run scrape:find-missing
```

This analyzes what's been scraped vs what's available and generates:
- `data/unscraped-urls.json` - List of 194 URLs to scrape
- Category breakdown showing where gaps are

### 2. Start Autonomous Scraper

```bash
npm run scrape:autonomous
```

This will:
- Process all 194 unscraped items sequentially
- Run in headless browser mode (background)
- Auto-select first option for configurable items
- Save after every 10 items
- Log everything to `data/autonomous-scraper.log`

**Estimated Time:** ~6-10 hours (2-3 items/minute)

### 3. Monitor Progress

In another terminal:

```bash
npm run scrape:monitor
```

Shows:
- Items processed / total
- Success rate
- Time elapsed
- ETA to completion
- Recent activity
- Failed URLs (if any)

## Files

### Input
- `data/unscraped-urls.json` - URLs to scrape (generated by find-missing)

### Output
- `data/cype-full.json` - Main database (updated incrementally)
- `data/autonomous-scraper.log` - Complete activity log
- `data/scraper-progress.json` - Progress tracker (for resuming)

### Scripts
- `scripts/find-unscraped-items.ts` - Identifies missing items
- `scripts/autonomous-batch-scraper.ts` - Core scraper
- `scripts/monitor-scraper.ts` - Progress monitor
- `scripts/start-autonomous-scraper.ts` - Startup wrapper

## Usage Examples

### Complete Workflow

```bash
# Step 1: Find what's missing
npm run scrape:find-missing

# Step 2: Start scraper in background
npm run scrape:autonomous &

# Step 3: Monitor (in another terminal)
npm run scrape:monitor

# Step 4: Check logs
tail -f data/autonomous-scraper.log
```

### Resume After Interruption

If scraper is stopped (Ctrl+C, crash, etc), just run again:

```bash
npm run scrape:autonomous
```

It will automatically resume from where it left off using `data/scraper-progress.json`.

### Reset and Start Fresh

```bash
# Remove progress file to start from beginning
rm data/scraper-progress.json

# Start scraper
npm run scrape:autonomous
```

## Configuration

Edit `scripts/autonomous-batch-scraper.ts` to adjust:

```typescript
const CONFIG = {
  batchSize: 10,           // Save after N items (default: 10)
  delayBetweenItems: 2000, // ms between items (default: 2000)
  maxRetries: 2,           // Retry attempts (default: 2)
};
```

## Auto-Selection Behavior

For pages with configurable options (dropdowns, radio buttons):

1. **Dropdowns**: Selects first option with a value
2. **Radio Buttons**: Selects first in each group
3. **Waits**: 500ms for dynamic updates after selection

This ensures complete coverage without manual intervention.

## Progress Tracking

The scraper saves progress after every item:

```json
{
  "startedAt": "2026-02-16T18:00:00Z",
  "lastUpdate": "2026-02-16T18:15:00Z",
  "totalItems": 194,
  "processedItems": 45,
  "successfulItems": 43,
  "failedItems": 2,
  "currentIndex": 45,
  "failedUrls": ["url1", "url2"]
}
```

## Error Handling

**Automatic Retries:**
- Each failed item is retried 2 times
- Failed URLs are logged in progress file

**Common Issues:**
- **Timeout**: Increased to 30s per page
- **Missing Table**: Logged and skipped
- **Network Error**: Retried automatically

**Manual Retry:**
After completion, check failed URLs and retry manually:

```bash
# View failed URLs
cat data/scraper-progress.json | jq '.failedUrls'

# Create retry list
cat data/scraper-progress.json | jq -r '.failedUrls[]' > data/retry-urls.txt

# Update unscraped-urls.json with just these URLs
# Then run scraper again
```

## Performance

**Speed:**
- 2-3 items per minute average
- Includes 2s delay between items (rate limiting)
- 194 items â‰ˆ 6-10 hours total

**Resource Usage:**
- Headless browser (low GPU usage)
- ~200-300 MB RAM
- Minimal CPU (background priority)

**Network:**
- ~1-2 MB per item
- Total: ~200-400 MB

## Integration

After scraping completes, the data is available in:

```typescript
import cypeFull from './data/cype-full.json';

// Now contains 2,049 + 194 = 2,243 items
console.log(cypeFull.items.length); // 2243
```

## Monitoring Commands

```bash
# Check progress
npm run scrape:monitor

# View live log
tail -f data/autonomous-scraper.log

# Count items scraped
cat data/cype-full.json | jq '.items | length'

# Check success rate
cat data/scraper-progress.json | jq '{success: .successfulItems, failed: .failedItems, rate: (.successfulItems / .processedItems * 100)}'
```

## Stopping

**Graceful Stop:**
```bash
# Press Ctrl+C in scraper terminal
# Progress is automatically saved
```

**Force Stop:**
```bash
pkill -f autonomous-batch-scraper
# Progress saved up to last completed item
```

## Troubleshooting

### Scraper Won't Start

```bash
# Check prerequisites
npm run scrape:find-missing

# Ensure unscraped-urls.json exists
ls -lh data/unscraped-urls.json
```

### Progress Not Updating

```bash
# Check if scraper is running
ps aux | grep autonomous-batch-scraper

# Check log for errors
tail -20 data/autonomous-scraper.log
```

### Items Still Missing After Completion

```bash
# Find what's still missing
npm run scrape:find-missing

# Check if new items were added to CYPE site
# Re-crawl URL list if needed
```

## Next Steps

After autonomous scraping completes:

1. **Verify Coverage**: Run `npm run scrape:find-missing` again
2. **Validate Data**: Check for price anomalies
3. **Update Frontend**: Rebuild price database
4. **Schedule Refreshes**: Monthly incremental scraping

## Related Documentation

- [CYPE Coverage Report](./CYPE_COVERAGE_REPORT.md)
- [Main Scraper Docs](./CYPE_SCRAPER.md)
- [Price Validation](./PRICE_VALIDATION.md)
