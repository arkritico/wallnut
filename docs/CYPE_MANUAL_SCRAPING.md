# üéØ CYPE Manual Scraping - Guia de Uso

## Overview

Ferramentas para scraping manual de URLs espec√≠ficas do CYPE que descobriste atrav√©s de navega√ß√£o.

---

## Op√ß√£o 1: Scraping de URLs Espec√≠ficas

### Uso R√°pido

```bash
# Scrape um √∫nico URL
npx tsx scripts/scrape-specific-urls.ts "https://geradordeprecos.info/obra_nova/..."

# Scrape m√∫ltiplos URLs
npx tsx scripts/scrape-specific-urls.ts \
  "https://geradordeprecos.info/obra_nova/Instalacoes/..." \
  "https://geradordeprecos.info/obra_nova/Revestimentos/..."
```

### Uso com Ficheiro

1. **Criar ficheiro de URLs** (um por linha):

```bash
# Editar data/my-urls.txt
code data/my-urls.txt
```

Conte√∫do exemplo:
```
https://geradordeprecos.info/obra_nova/Instalacoes/Eletricas/IEQ010.html
https://geradordeprecos.info/obra_nova/Instalacoes/Eletricas/IEQ020.html
# Coment√°rios come√ßam com #
https://geradordeprecos.info/obra_nova/Drenagem/ISB010.html
```

2. **Executar scraping:**

```bash
npx tsx scripts/scrape-specific-urls.ts --file data/my-urls.txt
```

3. **Output:**

Resultados salvos em: `data/cype-manual-scrape-2026-02-16.json`

---

## Op√ß√£o 2: Browser Interativo (Explora√ß√£o Guiada)

### Setup (primeira vez apenas)

```bash
npm install puppeteer
```

### Uso

```bash
npx tsx scripts/browse-and-scrape.ts
```

### Workflow

1. **Browser abre automaticamente** no site CYPE
2. **Navegas livremente** para encontrar categorias interessantes
3. **Usas comandos** na terminal:

```
> save      # Guardar p√°gina atual
> links     # Ver todos os links na p√°gina
> scrape    # Fazer scrape da p√°gina atual
> list      # Ver URLs guardados
> done      # Terminar e exportar
```

4. **URLs s√£o exportados** para ficheiro
5. **Podes depois fazer scrape** dessas URLs:

```bash
npx tsx scripts/scrape-specific-urls.ts --file data/cype-discovered-urls-2026-02-16.txt
```

---

## Exemplos Pr√°ticos

### Exemplo 1: Encontrar Instala√ß√µes El√©tricas

**M√©todo Browser:**

```bash
npx tsx scripts/browse-and-scrape.ts
```

1. Browser abre em `https://geradordeprecos.info/obra_nova/`
2. Clica em "Instala√ß√µes"
3. Clica em "El√©tricas"
4. Na terminal: `links` (mostra links da p√°gina)
5. Escolhe n√∫meros dos links interessantes: `1,3,5,7`
6. Terminal: `done`
7. URLs exportados para ficheiro

**Resultado:**
```
‚úÖ Exported 4 URLs to: data/cype-discovered-urls-2026-02-16.txt

üí° Para fazer scrape destas URLs:
   npx tsx scripts/scrape-specific-urls.ts --file data/cype-discovered-urls-2026-02-16.txt
```

### Exemplo 2: URLs que J√° Conheces

**Se j√° tens URLs espec√≠ficos:**

```bash
# Criar ficheiro
cat > data/electrical-urls.txt << 'EOF'
https://geradordeprecos.info/obra_nova/Instalacoes/Eletricas/Quadros/IEQ010.html
https://geradordeprecos.info/obra_nova/Instalacoes/Eletricas/Quadros/IEQ020.html
https://geradordeprecos.info/obra_nova/Instalacoes/Eletricas/Cabos/IEC015.html
EOF

# Scrape
npx tsx scripts/scrape-specific-urls.ts --file data/electrical-urls.txt
```

**Output:**
```
üöÄ Starting scrape of 3 URLs...

[1/3] üîç Scraping: https://geradordeprecos.info/.../IEQ010.html
   ‚úÖ IEQ010: Quadro de distribui√ß√£o para instala√ß√£o el√©trica...

[2/3] üîç Scraping: https://geradordeprecos.info/.../IEQ020.html
   ‚úÖ IEQ020: Quadro de prote√ß√£o com disjuntores...

[3/3] üîç Scraping: https://geradordeprecos.info/.../IEC015.html
   ‚úÖ IEC015: Cabo el√©trico H07V-U 2.5mm¬≤...

‚úÖ Scraped 3 items
üìÑ Saved to: data/cype-manual-scrape-2026-02-16.json
```

### Exemplo 3: Explorar e Depois Scrape

**Passo 1 - Explorar:**
```bash
npx tsx scripts/browse-and-scrape.ts
```

Navega e guarda URLs interessantes:
```
> links
üîó Links na p√°gina atual:

1. Quadros de distribui√ß√£o
   https://geradordeprecos.info/.../Quadros/
2. Cabos el√©ctricos
   https://geradordeprecos.info/.../Cabos/
3. Tomadas e interruptores
   https://geradordeprecos.info/.../Tomadas/

Guardar algum link? (n√∫meros separados por v√≠rgula): 1,2
‚úÖ Saved: Quadros de distribui√ß√£o
‚úÖ Saved: Cabos el√©ctricos

> done
‚úÖ Exported 2 URLs to: data/cype-discovered-urls-2026-02-16.txt
```

**Passo 2 - Scrape:**
```bash
npx tsx scripts/scrape-specific-urls.ts --file data/cype-discovered-urls-2026-02-16.txt
```

---

## Comandos do Browser Interativo

| Comando | Descri√ß√£o | Exemplo |
|---------|-----------|---------|
| `save` | Guardar URL da p√°gina atual | `> save` |
| `links` | Mostrar todos os links na p√°gina | `> links` |
| `scrape` | Fazer scrape da p√°gina atual | `> scrape` |
| `list` | Ver URLs guardados | `> list` |
| `done` | Terminar e exportar | `> done` |
| `help` | Mostrar ajuda | `> help` |

---

## Estrutura de Output

### Ficheiro JSON de Resultados

```json
{
  "metadata": {
    "exportDate": "2026-02-16T15:30:00.000Z",
    "totalItems": 3,
    "source": "manual-url-scrape",
    "urls": ["https://...", "https://..."]
  },
  "items": [
    {
      "code": "IEQ010",
      "description": "Quadro de distribui√ß√£o...",
      "category": "Instala√ß√µes el√©tricas",
      "unit": "Ud",
      "unitCost": 680,
      "breakdown": {
        "materials": 480,
        "labor": 170,
        "machinery": 30
      },
      "url": "https://..."
    }
  ]
}
```

---

## Tips & Tricks

### 1. Encontrar Categorias no Site CYPE

**Estrutura t√≠pica de URLs:**
```
https://geradordeprecos.info/obra_nova/
  ‚îú‚îÄ‚îÄ Instalacoes/
  ‚îÇ   ‚îú‚îÄ‚îÄ Eletricas/
  ‚îÇ   ‚îú‚îÄ‚îÄ Drenagem/
  ‚îÇ   ‚îî‚îÄ‚îÄ AVAC/
  ‚îú‚îÄ‚îÄ Revestimentos/
  ‚îú‚îÄ‚îÄ Estruturas/
  ‚îî‚îÄ‚îÄ ...
```

**Estrat√©gia:**
1. Come√ßa em `https://geradordeprecos.info/obra_nova/`
2. Explora sec√ß√µes principais (Instala√ß√µes, Revestimentos, etc.)
3. Guarda URLs de categorias inteiras
4. Ou guarda URLs de items espec√≠ficos

### 2. Validar URLs Antes de Scrape

```bash
# Testar um √∫nico URL primeiro
npx tsx scripts/scrape-specific-urls.ts "https://geradordeprecos.info/obra_nova/..."

# Se funcionar, fazer scrape em batch
npx tsx scripts/scrape-specific-urls.ts --file data/my-urls.txt
```

### 3. Combinar com Scrape Existente

```bash
# 1. Fazer manual scrape de categorias espec√≠ficas
npx tsx scripts/scrape-specific-urls.ts --file data/electrical-urls.txt

# Output: data/cype-manual-scrape-2026-02-16.json

# 2. Merge com dados existentes
# (pode fazer manualmente ou criar script de merge)
```

---

## Troubleshooting

### Erro: "Failed to extract item"

**Causa:** URL n√£o √© de um item individual, ou estrutura HTML mudou

**Solu√ß√£o:**
- Verifica que o URL √© de um item espec√≠fico (n√£o categoria)
- Tenta abrir o URL no browser primeiro
- Exemplo de URL correto: `...NAF010_Isolamento_termico.html`

### Browser Interativo n√£o abre

**Causa:** Puppeteer n√£o instalado

**Solu√ß√£o:**
```bash
npm install puppeteer
```

### Rate Limiting

**Causa:** Muitos requests muito r√°pido

**Solu√ß√£o:**
- O scraper j√° tem delay de 1s entre requests
- Se necess√°rio, espera alguns minutos e tenta novamente

---

## Workflow Recomendado

### Para Descobrir Novas Categorias

```bash
# 1. Explorar com browser
npx tsx scripts/browse-and-scrape.ts

# 2. Guardar URLs interessantes (comando 'save' ou 'links')

# 3. Exportar URLs (comando 'done')

# 4. Scrape URLs descobertos
npx tsx scripts/scrape-specific-urls.ts --file data/cype-discovered-urls-YYYY-MM-DD.txt
```

### Para Scrape de URLs Conhecidos

```bash
# 1. Criar ficheiro com URLs
code data/my-urls.txt

# 2. Scrape direto
npx tsx scripts/scrape-specific-urls.ts --file data/my-urls.txt
```

---

## Pr√≥ximos Passos

Depois de fazer scrape manual:

1. **Validar resultados:**
   ```bash
   cat data/cype-manual-scrape-2026-02-16.json | grep "code"
   ```

2. **Merge com base de dados principal:**
   - Copiar items do ficheiro manual para `data/cype-full.json`
   - Ou criar script de merge autom√°tico

3. **Refresh matcher database:**
   ```bash
   # Via c√≥digo
   import { refreshCypeDatabase } from './cype-matcher';
   refreshCypeDatabase();
   ```

---

**Quest√µes?** Experimenta primeiro com um URL de teste!
